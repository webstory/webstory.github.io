---
category: 2023
tags:
  ["2023", "dev", "ai", "llm", "llama", "chatgpt", "rwkv", "text_generation"]
---

# ë‚´ ì»´í“¨í„° ì•ˆì˜ ChatGPT, Raven

ë­”ê°€ ì—„ì²­ë‚œ ê±¸ ë³¸ ê²ƒ ê°™ë‹¤. [Raven - RNN ê¸°ë°˜ LLMì˜ ì—­ìŠµ (Alpacaì™€ ë¹„ìŠ·í•œ í€„ë¦¬í‹°, ë¹ ë¥¸ ë™ì‘](https://youtu.be/cNda9mJItHQ) ì´ë‹¤. ChatGPTë¥¼ ë‚´ ë¡œì»¬ ì»´í“¨í„°ì— ì‹¬ì„ ìˆ˜ ìˆëŠ” ë­ ê·¸ëŸ° ê±´ê°€ë³´ë‹¤.

ê¶ê¸ˆí•´ì„œ ë‹¹ì¥ ë™ì˜ìƒì˜ ë‚´ìš©ëŒ€ë¡œ ë”°ë¼í•´ë´¤ë‹¤.

## ë°ëª¨ í…ŒìŠ¤íŠ¸

í—ˆê¹…í˜ì´ìŠ¤ëŠ” ëª¨ë¸ë§Œ ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ê²Œ ì•„ë‹ˆë¼ ëª¨ë¸ì„ ëŒë ¤ë³¼ ìˆ˜ ìˆëŠ” ê³µê°„ë„ ì œê³µí•œë‹¤. [í—ˆê¹…í˜ì´ìŠ¤ ìŠ¤í˜ì´ìŠ¤](https://huggingface.co/spaces) ë¼ëŠ” ê³³ì¸ë°, ìœ ë£Œë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ë˜ëŠ” ì»¤ë®¤ë‹ˆí‹° ì§€ì›(ì¢‹ì•„ìš” ê°™ì€ ê°œë…ì¸ ê²ƒ ê°™ë‹¤)ì„ ë°›ìœ¼ë©´ ë¬´ë£Œë¡œ ê³ ì„±ëŠ¥ ì»´í“¨íŒ… ìì›(GPU)ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

[https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B)

ì´ê²Œ ë°ëª¨ í˜ì´ì§€ì´ë‹¤. ChatGPTë¥¼ ìƒê°í•˜ê³  ëŒë ¤ë³´ë©´, ë­”ê°€ ì•„ì‰½ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "Make me a 99 bottles of beer for python" ì´ë¼ê³  í•˜ë©´ ì½”ë“œë¥¼ ë§Œë“¤ë‹¤ê°€ ë§ˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. í† í° ì œí•œ ë•Œë¬¸ì¸ë°, ë¡œì»¬ì—ì„œ ëŒë¦°ë‹¤ë©´ ì´ ì œí•œì„ í’€ ìˆ˜ ìˆì„ ê±°ë¼ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤.

## ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜

### ë°ëª¨ ì•± ë‹¤ìš´ë¡œë“œ

[ë°ëª¨ í˜ì´ì§€](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B)ì— ì ‘ì†í•œë‹¤.

ì˜¤ë¥¸ìª½ êµ¬ì„ì˜ `...` ë¥¼ í´ë¦­í•´ `Clone repository`ë¥¼ í´ë¦­í•œë‹¤.

![Clone repository](/assets/images/2023/2023-04-24-Raven%20LLM/clone-repository.png)

![Alt text](/assets/images/2023/2023-04-24-Raven%20LLM/clone-repository-dialog.png)

ë°ëª¨ í˜ì´ì§€ ìì²´ì—ëŠ” ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ git lfsëŠ” í•„ìš”ì—†ë‹¤.

```sh
git clone https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B
```

### ì‹¤í–‰ í™˜ê²½ ì„¤ì •

ì°¸ê³ ë¡œ íŒŒì´ì¬ ë²„ì „ì€ 3.10.6ì´ë‹¤. python 3.11 ë²„ì „ì€ ì•„ë§ˆ ì˜¤ë¥˜ê°€ ë‚  ê²ƒì´ë‹¤. ì•„ì§ ë¨¸ì‹ ëŸ¬ë‹ íŒ¨í‚¤ì§€ë“¤ì´ ìµœì‹  ë²„ì „ íŒŒì´ì¬ì—ì„œëŠ” ë¬¸ì œê°€ ìƒê¸´ë‹¤. ê·¸ë‚˜ë§ˆ 3.8ì—ì„œ 3.10ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œê°€ ë˜ì—ˆìœ¼ë‹ˆ ë‹¤í–‰ì´ë‹¤.

```sh
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

ì´ ì‘ì—…ì€ í•œë²ˆë§Œ í•˜ë©´ ëœë‹¤. ë‹¤ìŒì— ë‹¤ì‹œ Ravenì„ ì‹¤í–‰ì‹œí‚¤ë ¤ê³  í•  ë•ŒëŠ”

```sh
source venv/bin/activate
```

ë§Œ ì‹¤í–‰í•˜ë©´ ëœë‹¤. í”„ë¡¬í”„íŠ¸ ì•ì— `(venv)` ê°€ í‘œì‹œë˜ëŠ” ê±¸ í™•ì¸í•˜ì.

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

ë‚´ ê·¸ë˜í”½ì¹´ë“œë¡œëŠ” VRAMì´ ì´ˆê³¼ë¼ì„œ ê·¸ëŒ€ë¡œëŠ” ëŒë¦´ ìˆ˜ê°€ ì—†ë‹¤. ë‹¤ë¥¸ ëª¨ë¸ì„ ì°¾ì•„ë´ì•¼ í•œë‹¤.

[https://huggingface.co/BlinkDL/rwkv-4-raven](https://huggingface.co/BlinkDL/rwkv-4-raven) ì— ì ‘ì†í•œë‹¤.

Files and versions ë¥¼ í´ë¦­í•œë‹¤.

![Download models](../../assets/images/2023/2023-04-24-Raven%20LLM/download-models-1.png)

ëª©ë¡ ì¤‘ì—ì„œ íŒŒì¼ ìš©ëŸ‰ì´ ë³¸ì¸ GPU VRAMë³´ë‹¤ ì‘ì€(ì•„ë§ˆë„) ëª¨ë¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œë°›ëŠ”ë‹¤.

ë‹¤ìš´ë¡œë“œë°›ì€ íŒŒì¼ì„ ìœ„ì—ì„œ í´ë¡ í•œ ë¦¬í¬ì§€í† ë¦¬ì˜ `/models` í´ë”ì— ë³µì‚¬í•œë‹¤. í´ë”ê°€ ì—†ì„ í…ë° ìƒˆë¡œ ë§Œë“ ë‹¤.

### íŒŒë¼ë¯¸í„° ìˆ˜ì •

`title` ì„ ë‹¤ìš´ë°›ì€ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë³€ê²½í•œë‹¤.

```python
title = "RWKV-4-Raven-3B-v9-Eng99%-Other1%-20230411-ctx4096"
```

`model_path` ë¥¼ ìˆ˜ì •í•œë‹¤.

```python
model_path = f"models/{title}.pth"
```

ê¸°ì¡´ì—ëŠ” ì´ë ‡ê²Œ ë˜ì–´ ìˆì—ˆì„ ê²ƒì´ë‹¤.

```python
model_path = hf_hub_download(repo_id="BlinkDL/rwkv-4-raven", filename=f"{title}.pth")
```

`hf_hub_download`ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¡œ, í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ëŠ” ë„êµ¬ì´ë‹¤. ì›í•˜ë©´ íŒŒì¼ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ëŒ€ì‹  ì´ê±¸ ì‚¬ìš©í•´ë„ ëœë‹¤.

strategyëŠ” ì–´ì©Œë©´ ìˆ˜ì •í•´ì•¼ í•  ì§€ë„ ëª¨ë¥¸ë‹¤. ë³¸ì¸ VRAMì´ ì‘ì•„ì„œ(ì‘ì—… ê´€ë¦¬ìë¡œ í™•ì¸í•´ë³´ì. `Ctrl+Shift+ESC` ëˆ„ë¥´ë©´ ë‚˜ì˜¨ë‹¤) ì‹¤í–‰ì´ ì•ˆ ë˜ë©´ https://pypi.org/project/rwkv/ ì— ì íŒ ì„¤ëª…ì— ë”°ë¼ `strategy`ë¥¼ ìˆ˜ì •í•œë‹¤.

```python
########################################################################################################
#
# Use '/' in model path, instead of '\'. Use ctx4096 models if you need long ctx.
#
# fp16 = good for GPU (!!! DOES NOT support CPU !!!)
# fp32 = good for CPU
# bf16 = worse accuracy, supports CPU
# xxxi8 (example: fp16i8, fp32i8) = xxx with int8 quantization to save 50% VRAM/RAM, slower, slightly less accuracy
#
# We consider [ln_out+head] to be an extra layer, so L12-D768 (169M) has "13" layers, L24-D2048 (1.5B) has "25" layers, etc.
# Strategy Examples: (device = cpu/cuda/cuda:0/cuda:1/...)
# 'cpu fp32' = all layers cpu fp32
# 'cuda fp16' = all layers cuda fp16
# 'cuda fp16i8' = all layers cuda fp16 with int8 quantization
# 'cuda fp16i8 *10 -> cpu fp32' = first 10 layers cuda fp16i8, then cpu fp32 (increase 10 for better speed)
# 'cuda:0 fp16 *10 -> cuda:1 fp16 *8 -> cpu fp32' = first 10 layers cuda:0 fp16, then 8 layers cuda:1 fp16, then cpu fp32
#
# Basic Strategy Guide: (fp16i8 works for any GPU)
# 100% VRAM = 'cuda fp16'                   # all layers cuda fp16
#  98% VRAM = 'cuda fp16i8 *1 -> cuda fp16' # first 1 layer  cuda fp16i8, then cuda fp16
#  96% VRAM = 'cuda fp16i8 *2 -> cuda fp16' # first 2 layers cuda fp16i8, then cuda fp16
#  94% VRAM = 'cuda fp16i8 *3 -> cuda fp16' # first 3 layers cuda fp16i8, then cuda fp16
#  ...
#  50% VRAM = 'cuda fp16i8'                 # all layers cuda fp16i8
#  48% VRAM = 'cuda fp16i8 -> cpu fp32 *1'  # most layers cuda fp16i8, last 1 layer  cpu fp32
#  46% VRAM = 'cuda fp16i8 -> cpu fp32 *2'  # most layers cuda fp16i8, last 2 layers cpu fp32
#  44% VRAM = 'cuda fp16i8 -> cpu fp32 *3'  # most layers cuda fp16i8, last 3 layers cpu fp32
#  ...
#   0% VRAM = 'cpu fp32'                    # all layers cpu fp32
#
# Use '+' for STREAM mode, which can save VRAM too, and it is sometimes faster
# 'cuda fp16i8 *10+' = first 10 layers cuda fp16i8, then fp16i8 stream the rest to it (increase 10 for better speed)
#
# Extreme STREAM: 3G VRAM is enough to run RWKV 14B (slow. will be faster in future)
# 'cuda fp16i8 *0+ -> cpu fp32 *1' = stream all layers cuda fp16i8, last 1 layer [ln_out+head] cpu fp32
#
# ########################################################################################################
```

ë§ˆì§€ë§‰ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë°ëª¨ë³´ë‹¤ ë” ë§ì€ í† í° ìˆ˜ë¥¼ ì›í•˜ë¯€ë¡œ, token ìˆ˜ë¥¼ 2000 ê°œë¡œ ì¡°ì •í•œë‹¤. ì•„ë˜ ì„¸ ë¶€ë¶„ì„ ë°”ê¾¸ë©´ ëœë‹¤.

```python
def evaluate(
    instruction,
    input=None,
    token_count=2000, # 200 â†’ 2000
    temperature=1.0,
    top_p=0.7,
    presencePenalty = 0.1,
    countPenalty = 0.1,
):

```

```python
            with gr.Column():
                instruction = gr.Textbox(lines=2, label="Instruction", value="Tell me about ravens.")
                input = gr.Textbox(lines=2, label="Input", placeholder="none")
                # Edit this below. 200 -> 2000, 150 -> 2000
                token_count = gr.Slider(10, 2000, label="Max Tokens", step=10, value=2000)
                temperature = gr.Slider(0.2, 2.0, label="Temperature", step=0.1, value=1.2)
                top_p = gr.Slider(0.0, 1.0, label="Top P", step=0.05, value=0.5)
                presence_penalty = gr.Slider(0.0, 1.0, label="Presence Penalty", step=0.1, value=0.4)
                count_penalty = gr.Slider(0.0, 1.0, label="Count Penalty", step=0.1, value=0.4)

```

## ì•± ì‹¤í–‰

ì´ì œ `python app.py` ë¥¼ ì‹¤í–‰í•œë‹¤.

```sh
(venv)$ python app.py
```

ë§¨ ë§ˆì§€ë§‰ì— í‘œì‹œë˜ëŠ” http://127.0.0.1:7860 ì„ ë¸Œë¼ìš°ì €ë¡œ ì ‘ì†í•˜ë©´ ì´ì œ 2000í† í°ì§œë¦¬ ë‚˜ë§Œì˜ ChatGPTê°€ íƒ„ìƒí•œë‹¤.

## Bug?

í† í° ìˆ˜ë¥¼ ì–µì§€ë¡œ ì—´ ë°°ë¡œ ì˜¬ë¦° íƒ“ì— ìƒì„±ì´ ì© ë§¤ë„ëŸ½ì§€ëŠ” ëª»í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê½¤ ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ì¸ `B+ Tree`ì•Œê³ ë¦¬ì¦˜ì„ ì¨ë‹¬ë¼ê³  í•˜ë©´ ì—´ì‹¬íˆ ì“°ë‹¤ê°€ ì¤‘ê°„ì— ì• ê°€ ë§›ì´ ê°€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```
write me a b+ tree algorithm in c
```

## ì‚¬ìš© í›„ê¸°

ê·¸ë¦¬ê³  ì½”ë”© ì™¸ì˜ ì§ˆë¬¸ì€ ë‚´ê°€ ì˜ í•  ì¤„ ëª°ë¼ì„œ ê·¸ëƒ¥ ë‘ì—ˆë‹¤. ì‚¬ì‹¤ ê·¸ëƒ¥ ì¼ìƒì ì¸ ì§ˆë¬¸ì€ [Bing Chat](http://bing.com/chat) ì—ì„œ í•˜ë©´ ëœë‹¤(ì—£ì§€ ë¸Œë¼ìš°ì €ê°€ í•„ìˆ˜ë¼ëŠ” ì ì€ ì¢€ ì•„í”Œ ìˆ˜ë„ ìˆê² ë‹¤. ë¦¬ëˆ…ìŠ¤ ì‚¬ìš©ìëŠ”?)

ì½”ë”© ì¤‘ì— ìì˜í•˜ë©´ì„œë„ ìœ ìš©í•œ ë„ì›€ì„ ì£¼ëŠ” ê±´ ê¹ƒí—ˆë¸Œ ì½”íŒŒì¼ëŸ¿(https://github.com/features/copilot) ì´ ë” ë‚«ë‹¤. ì£¼ì„ìœ¼ë¡œ ë­˜ ì›í•˜ëŠ”ì§€ ì“°ë©´ ì•Œì•„ì„œ ì½”ë“œë¥¼ ë°‘ì— ë°•ì•„ì£¼ë‹ˆê¹Œ. ë§ˆìŒì— ì•ˆ ë“¤ë©´ `Ctrl+Enter` ëˆŒëŸ¬ì„œ ë‹¤ë¥¸ ì œì•ˆì„ ë³¼ ìˆ˜ë„ ìˆë‹¤. ì£¼ì„ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ” ì •ë„ì—ì„œ ëë‚˜ëŠ” ê²ƒë„ ì•„ë‹ˆë¼ ë­”ê°€ ë‹¨ìˆœë°˜ë³µì‘ì—… ê°™ì€ ê±¸ í•˜ê³  ìˆìœ¼ë©´ ì½”íŒŒì¼ëŸ¿ì´ ì•Œì•„ì„œ ë‚˜ë¨¸ì§€ ë‚´ìš©ì„ ë‹¤ ì ì–´ì£¼ê¸° ë•Œë¬¸ì— ë‚œ ê·¸ëƒ¥ `tab`ë§Œ ëˆ„ë¥´ë©´ ëœë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ, ì–˜ëŠ” ì—°ì†ëœ ì§ˆë¬¸ì„ í•˜ê¸°ê°€ ì–´ë µë‹¤. input ì°½ ì•ˆì—ë‹¤ê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´ ì–´ë–»ê²Œë“  ë¹„ìŠ·í•˜ê²Œ í‰ë‚´ë‚¼ ìˆ˜ ìˆì§€ë§Œ ì—­ì‹œ ëŒ€í™”í˜• ì½”ë“œ ìƒì„±(?)ì€ ChatGPTê°€ ì œì¼ ë‚«ë‹¤.

ê·¸ë˜ë„ ì¼í•  ë•Œ ì¸í„°ë„·ì„ í‹€ì–´ë§‰ì•„ë²„ë¦¬ëŠ”(ì†Œë¬¸ë§Œ ë“¤ì—ˆë‹¤) ê¸ˆìœµê¶Œ ê°œë°œìë“¤ì—ê²ŒëŠ” ì´ëŸ° ê²Œ í•„ìš”í•˜ì§€ ì•Šì„ê¹Œ ìƒê°í•´ë³¸ë‹¤. ì–´ì§¸ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë¡œì§ ì§œëŠ”ë° RTX4090 ì´ í•„ìš”í•œì§€ ì„¤ë“í•˜ëŠ” ì¼ì€ ì—¬ëŸ¬ë¶„ì—ê²Œ ë§¡ê¸´ë‹¤ ğŸ˜ğŸ˜ğŸ˜

## ë²ˆì™¸: êµ¬ê¸€ ì½”ë©?

êµ¬ê¸€ ì½”ë©ì—ì„œë„ ëŒë¦´ ìˆ˜ ìˆë‹¤.

### ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±

![Create a new document](/assets/images/2023/2023-04-24-Raven%20LLM/colab-new-doc.png)

### app.py ì´ì‹

app.py ì½”ë“œë¥¼ ì½”ë© ë…¸íŠ¸ë¶ì— ë³µì‚¬í•˜ê³  ìœ„ì— ì½”ë“œ ë¸”ëŸ­ì„ í•˜ë‚˜ ì¶”ê°€í•œ ë’¤ requirements.txt íŒŒì¼ì˜ ë‚´ìš©ì„ ë…¸íŠ¸ë¶ ìƒë‹¨ì— pip ì„¤ì¹˜ ëª…ë ¹ë§Œ ì¶”ê°€í•´ì¤€ë‹¤.

```python
!pip install torch ninja tokenizers rwkv==0.6.2 pynvml huggingface_hub "gradio>=3.17.1"
```

### 20B_tokenizer.json ë³µì‚¬

ì½”ë©ì˜ íŒŒì¼ íƒ­ì„ ì—´ì–´ 20B_tokenizer.json ì„ ì—…ë¡œë“œí•œë‹¤. ë“œë˜ê·¸ ë“œë¡­í•˜ë©´ ëœë‹¤.

![Copy 20B_Tokenizer.json](/assets/images/2023/2023-04-24-Raven%20LLM/copy-tokenizer-json.png)

ì´ íŒŒì¼ì€ ì„¸ì…˜ì´ ì¢…ë£Œë˜ë©´ ìë™ìœ¼ë¡œ ì‚­ì œë˜ë¯€ë¡œ ì½”ë©ì„ ì¼¤ ë•Œë§ˆë‹¤ ì´ ì‘ì—…ì„ ë°˜ë³µí•´ì¤˜ì•¼ í•œë‹¤. ê·€ì°®ìœ¼ë©´ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°í•´ì„œ ê±°ê¸° ì˜¬ë ¤ë‘ì„¸ìš”.

### íŒŒë¼ë¯¸í„° íŠœë‹

í•˜ì§€ë§Œ ì´ê²ƒë§Œ í•˜ë©´ ì™ ì§€ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ë¥¼ ê³¼í•˜ê²Œ ì“°ë‹¤ê°€ ëŸ°íƒ€ì„ì´ ë‹¤ìš´ëœë‹¤. í—ˆê¹…í˜ì´ìŠ¤ë³´ë‹¤ ë¦¬ì†ŒìŠ¤ë¥¼ ì§œê²Œ ì£¼ëŠ” ê²Œ í™•ì‹¤í•œ ê²ƒ ê°™ë‹¤. ê·¸ëŸ°ë° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ë³´ë©´ GPU ë©”ëª¨ë¦¬ëŠ” ëŸ°íƒ€ì„ì´ ë»—ì„ ë•Œê¹Œì§€ë„ ìƒë‹¹íˆ ì—¬ìœ ê°€ ìˆëŠ” ëª¨ìŠµì„ ë³´ì˜€ë‹¤.

ê¸°ë³¸ ëª¨ë¸ë„ ë©”ëª¨ë¦¬ë¥¼ í¬ê²Œ ì°¨ì§€í•˜ë¯€ë¡œ ë‹¤ìš´ë°›ì„ ëª¨ë¸ íŒŒì¼ì„ ìˆ˜ì •í•œë‹¤.
title ë³€ìˆ˜ë¥¼ ì°¾ì•„ ì•„ë˜ì™€ ê°™ì´ ë°”ê¿”ì¤€ë‹¤.

```python
title = "RWKV-4-Raven-3B-v9-Eng99%-Other1%-20230411-ctx4096"
```

strategyë¥¼ ìˆ˜ì •í•œë‹¤.

ì›ë˜ ì½”ë“œì— ìˆëŠ” `model = RWKV( ... )`ë¶€ë¶„ì„ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •í•œë‹¤.

```python
model = RWKV(model=model_path, strategy='cuda fp16')
```

ë‹¤ìŒ, ì½”ë“œ ë§¨ ì•„ë˜ gradio ë¶€ë¶„ë„ ìˆ˜ì •í•œë‹¤. `demo.launch(share=False)` ë¥¼ `demo.launch(share=True)` ìœ¼ë¡œ ìˆ˜ì •í•œë‹¤.

```python
demo.launch(share=True)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë°ëª¨ë³´ë‹¤ ë” ë§ì€ í† í° ìˆ˜ë¥¼ ì›í•˜ë¯€ë¡œ, token ìˆ˜ë¥¼ 2000 ê°œë¡œ ì¡°ì •í•œë‹¤. ì•„ë˜ ì„¸ ë¶€ë¶„ì„ ë°”ê¾¸ë©´ ëœë‹¤.

```python
def evaluate(
    instruction,
    input=None,
    token_count=2000, # 200 â†’ 2000
    temperature=1.0,
    top_p=0.7,
    presencePenalty = 0.1,
    countPenalty = 0.1,
):

```

```python
            with gr.Column():
                instruction = gr.Textbox(lines=2, label="Instruction", value="Tell me about ravens.")
                input = gr.Textbox(lines=2, label="Input", placeholder="none")
                # Edit this below. 200 -> 2000, 150 -> 2000
                token_count = gr.Slider(10, 2000, label="Max Tokens", step=10, value=2000)
                temperature = gr.Slider(0.2, 2.0, label="Temperature", step=0.1, value=1.2)
                top_p = gr.Slider(0.0, 1.0, label="Top P", step=0.05, value=0.5)
                presence_penalty = gr.Slider(0.0, 1.0, label="Presence Penalty", step=0.1, value=0.4)
                count_penalty = gr.Slider(0.0, 1.0, label="Count Penalty", step=0.1, value=0.4)

```

ì´ì œ `Ctrl+F9` ëˆŒëŸ¬ì„œ ì „ì²´ ì…€ì„ ì‹¤í–‰í•œë‹¤. ëŸ°íƒ€ì„ì´ ë‹¤ìš´ë  ìˆ˜ë„ ìˆëŠ”ë° ìë™ìœ¼ë¡œ ì¬ì ‘ì†í•˜ë¯€ë¡œ ë‹¤ì‹œ `Ctrl+F9` ëˆ„ë¥´ë©´ ì–´ì©Œë©´... ì‘ë™í•œë‹¤. ì‹œìŠ¤í…œ RAMì„ ê±°ì˜ ë‹¤ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì–´ì©” ìˆ˜ê°€ ì—†ë‹¤.

ì‹¤í–‰ í›„, Running on public URL: https://e1fe7ba66d19932684.gradio.live ê³¼ ê°™ì€ ì¤„ì„ ì°¾ì•„ í´ë¦­í•˜ë©´ ëœë‹¤.
